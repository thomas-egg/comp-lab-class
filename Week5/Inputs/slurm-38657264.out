srun: error: ioctl(TIOCGWINSZ): Inappropriate ioctl for device
srun: error: Not using a pseudo-terminal, disregarding --pty option
srun: error: CPU binding outside of job step allocation, allocated CPUs are: 0x500000010000.
srun: error: Task launch for StepId=38657264.0 failed on node cs043: Unable to satisfy cpu bind request
srun: error: Application launch failed: Unable to satisfy cpu bind request
srun: Job step aborted
                      :-) GROMACS - gmx mdrun, 2018.3 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov      Paul Bauer     Herman J.C. Berendsen
    Par Bjelkmar    Aldert van Buuren   Rudi van Drunen     Anton Feenstra  
  Gerrit Groenhof    Aleksei Iupinov   Christoph Junghans   Anca Hamuraru   
 Vincent Hindriksen Dimitrios Karkoulis    Peter Kasson        Jiri Kraus    
  Carsten Kutzner      Per Larsson      Justin A. Lemkul    Viveca Lindahl  
  Magnus Lundborg   Pieter Meulenhoff    Erik Marklund      Teemu Murtola   
    Szilard Pall       Sander Pronk      Roland Schulz     Alexey Shvetsov  
   Michael Shirts     Alfons Sijbers     Peter Tieleman    Teemu Virolainen 
 Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2017, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2018.3
Executable:   /share/apps/gromacs/2018.3/openmpi/intel/bin/gmx_mpi
Data prefix:  /share/apps/gromacs/2018.3/openmpi/intel
Working dir:  /scratch/work/courses/CHEM-GA-2671-2023fa/students/tje3676/comp-lab-class/Week5/Inputs
Command line:
  gmx_mpi mdrun -s adp -multidir T300/ T363 T440/ -deffnm adp_exchange3temps -replex 50

The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
Compiled SIMD: None, but for this host/run AVX_512 might be better (see log).
The current CPU can measure timings more accurately than the code in
gmx mdrun was configured to use. This might affect your simulation
speed as accurate timings are needed for load-balancing.
Please consider rebuilding gmx mdrun with the GMX_USE_RDTSCP=ON CMake option.
Reading file adp.tpr, VERSION 2018.3 (single precision)
Reading file adp.tpr, VERSION 2018.3 (single precision)

This is simulation 1 out of 3 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
Reading file adp.tpr, VERSION 2018.3 (single precision)

This is simulation 0 out of 3 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

This is simulation 2 out of 3 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity

NOTE: This file uses the deprecated 'group' cutoff_scheme. This will be
removed in a future release when 'verlet' supports all interaction forms.


NOTE: This file uses the deprecated 'group' cutoff_scheme. This will be
removed in a future release when 'verlet' supports all interaction forms.


NOTE: This file uses the deprecated 'group' cutoff_scheme. This will be
removed in a future release when 'verlet' supports all interaction forms.


-------------------------------------------------------
Program:     gmx mdrun, version 2018.3
Source file: src/programs/mdrun/repl_ex.cpp (line 228)
MPI rank:    0 (out of 3)

Fatal error:
The properties of the 3 systems are all the same, there is nothing to exchange

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------

-------------------------------------------------------
ogram:     gmx mdrun, version 2018.3
Source file: src/programs/mdrun/repl_ex.cpp (line 228)
MPI rank:    1 (out of 3)

Fatal error:
The properties of the 3 systems are all the same, there is nothing to exchange

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------

-------------------------------------------------------
Program:     gmx mdrun, version 2018.3
Source file: src/programs/mdrun/repl_ex.cpp (line 228)
MPI rank:    2 (out of 3)

Fatal error:
The properties of the 3 systems are all the same, there is nothing to exchange

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 2 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[cs043.hpc.nyu.edu:2954402] 2 more processes have sent help message help-mpi-api.txt / mpi-abort
[cs043.hpc.nyu.edu:2954402] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
